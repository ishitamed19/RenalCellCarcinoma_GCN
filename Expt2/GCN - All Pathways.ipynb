{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsa04530 .csv\n",
      "166 914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spektral/utils/convolution.py:30: RuntimeWarning: divide by zero encountered in power\n",
      "  degrees = np.power(np.array(A.sum(1)), k).flatten()\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0609 17:51:54.985846 4561358272 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  1\n",
      "Fold:  2\n",
      "Fold:  3\n",
      "Fold:  4\n",
      "Fold:  5\n",
      "Fold:  6\n",
      "Fold:  7\n",
      "Fold:  8\n",
      "Fold:  9\n",
      "Fold:  10\n",
      "Fold:  11\n",
      "Fold:  12\n",
      "Fold:  13\n",
      "Fold:  14\n",
      "Fold:  15\n",
      "Fold:  16\n",
      "Fold:  17\n",
      "Fold:  18\n",
      "Fold:  19\n",
      "Fold:  20\n",
      "Fold:  21\n",
      "Fold:  22\n",
      "Fold:  23\n",
      "Fold:  24\n",
      "Fold:  25\n",
      "Fold:  26\n",
      "Fold:  27\n",
      "Fold:  28\n",
      "Fold:  29\n",
      "Fold:  30\n",
      "['hsa04530 .csv', 0.9468628359064507, 0.9299685327119558, 0.9467936859228994, 0.9467936859228994, 0.9467936859228994, 0.9467936859228994]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spektral\n",
    "import os\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from scipy import sparse\n",
    "from spektral.layers import GraphConv\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from spektral.layers import ChebConv\n",
    "import gc\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "metric_scores_per_pathway = []\n",
    "pathways_not_used = [] #for those having <10 gene features\n",
    "\n",
    "kegg_pathways_path = '/Users/ishitamed/Desktop/IIITH/Data/KEGG_csv/'\n",
    "rcc_dataset_path = '/Users/ishitamed/Downloads/GCN_Dataset/CSV/'\n",
    "\n",
    "\n",
    "############## LOAD INPUT DATASET ##########################\n",
    "def load_dataset(path, filename, transpose=True):\n",
    "\t'''\n",
    "\t\tLoads the dataset and converts into its transpose with appropriate columns\n",
    "\t'''\n",
    "\tdf = pd.read_csv(os.path.join(path, filename))\n",
    "\tdf.rename(columns={\"Unnamed: 0\": \"pid\"}, inplace=True)\n",
    "\tif transpose:\n",
    "\t\tdf = df.astype({\"pid\": str})\n",
    "\t\tdf = df.T\n",
    "\t\tnew_header = df.iloc[0] \n",
    "\t\tdf = df[1:]\n",
    "\t\tdf.columns = new_header\n",
    "\treturn df\n",
    "\n",
    "df_kirp = load_dataset(rcc_dataset_path,'KIRP_290_tumors_log_transformed.csv',transpose=True)\n",
    "df_kirc = load_dataset(rcc_dataset_path,'KIRC_518_tumors_log_transformed.csv',transpose=True)\n",
    "df_kich = load_dataset(rcc_dataset_path,'KICH_81_tumors_log_transformed.csv',transpose=True)\n",
    "df_kirp['y'] = 0\n",
    "df_kirc['y'] = 1\n",
    "df_kich['y'] = 2\n",
    "data = pd.concat([df_kirp, df_kirc, df_kich])   \n",
    "\n",
    "del df_kirp\n",
    "del df_kirc\n",
    "del df_kich\n",
    "##########################################################\n",
    "\n",
    "f1_weighted_per_fold = []\n",
    "f1_macro_per_fold = []\n",
    "f1_micro_per_fold = []\n",
    "testacc_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "\n",
    "f1_weighted_per_level = []\n",
    "f1_macro_per_level = []\n",
    "f1_micro_per_level = []\n",
    "testacc_per_level = []\n",
    "precision_per_level=[]\n",
    "recall_per_level=[]\n",
    "\n",
    "##########################################################\n",
    "\n",
    "for file in ['hsa04530 .csv']:\n",
    "\n",
    "\n",
    "\tprint(file)\n",
    "\t# if file in os.path.join(\"/content/drive/My Drive/IIITH/GCN_KEGG/GCN_pathway_output_csv\"):\n",
    "\t# \tcontinue\n",
    "\t\n",
    "\tpathway = pd.read_csv(os.path.join(kegg_pathways_path,file))\n",
    "\tpathway.rename(columns={\"Unnamed: 0\": \"idx\"}, inplace=True)\n",
    "\t\n",
    "\tgenes_used = set()\n",
    "\n",
    "\tfor i in range(len(pathway)):\n",
    "\t\tgenes_used.add(pathway.iloc[i]['from'][4:])\n",
    "\t\tgenes_used.add(pathway.iloc[i]['to'][4:])\n",
    "\n",
    "\tto_remove = []\n",
    "\tfor gene in genes_used:\n",
    "\t\tif gene not in data.columns:\n",
    "\t\t\tto_remove.append(gene)\n",
    "\n",
    "\tfor gene in to_remove:\n",
    "\t\tgenes_used.remove(gene)\n",
    "\n",
    "\tgenes_used = list(genes_used)\n",
    "\n",
    "\tfor gene in to_remove:\n",
    "\t\tpathway = pathway[pathway['from']!=(\"hsa:\"+str(gene))]\n",
    "\t\tpathway = pathway[pathway['to']!=(\"hsa:\"+str(gene))]\n",
    "\n",
    "\tnodes = len(genes_used)\n",
    "\tedges = len(pathway)\n",
    "\tprint(nodes, edges)\n",
    "\n",
    "\tif(nodes<10):\n",
    "\t\tpathways_not_used.append(file)\n",
    "\t\tcontinue\n",
    "\n",
    "\tgenes_used.sort()\n",
    "\n",
    "\n",
    "\t# dict to map gene_id to node_number\n",
    "\tnode_map = {}\n",
    "\tcount = 0\n",
    "\tfor gene in genes_used:\n",
    "\t\tnode_map[(\"hsa:\"+str(gene))] = count\n",
    "\t\tcount += 1\n",
    "\n",
    "\t# CREATE ADJACENCY MATRIX\n",
    "\tadjacency_matrix = np.zeros((nodes,nodes))\n",
    "\tfor i in range(edges):\n",
    "\t\tn1 = pathway.iloc[i]['from']\n",
    "\t\tn2 = pathway.iloc[i]['to']\n",
    "\t\tn1 = node_map[n1]\n",
    "\t\tn2 = node_map[n2]\n",
    "\t\tadjacency_matrix[n1][n2] = 1\n",
    "\n",
    "\tA = sparse.csr_matrix(adjacency_matrix)\n",
    "\n",
    "\tassert adjacency_matrix.shape[0]==nodes #sanity check\n",
    "\tassert edges==len(pathway)\n",
    "\n",
    "\t# CREATE NODE FEATURES MATRIX\n",
    "\tX = data[genes_used]\n",
    "\tX = X.to_numpy()\n",
    "\tX = X.T\n",
    "\tassert X.shape[0]==nodes\n",
    "\n",
    "\t# CREATE TARGET LABELS\n",
    "\tOneHot = False\n",
    "\tif OneHot:\n",
    "\t\ty = []\n",
    "\t\tfor i in data['y']:\n",
    "\t\t\tif i==0:\n",
    "\t\t\t\ty.append([1,0,0])\n",
    "\t\t\telif i==1:\n",
    "\t\t\t\ty.append([0,1,0])\n",
    "\t\t\telif i==2:\n",
    "\t\t\t\ty.append([0,0,1])\n",
    "\telse:\n",
    "\t\ty = data['y']\n",
    "\n",
    "\ty = np.asarray(y)\n",
    "\n",
    "\t# BUILDING MODEL\n",
    "\t# Parameters\n",
    "\tl2_reg = 5e-4         # Regularization rate for l2\n",
    "\tlearning_rate = 5e-4  # Learning rate for SGD\n",
    "\tbatch_size = 32       # Batch size\n",
    "\tepochs = 50         # Number of training epochs\n",
    "\tes_patience = 0      # Patience fot early stopping\n",
    "\tchannels = 16           # Number of channels in the first layer\n",
    "\tK = 2  \n",
    "\tn_out = 3\n",
    "\n",
    "\tfltr = ChebConv.preprocess(A).astype('f4')\n",
    "\tassert fltr.shape==adjacency_matrix.shape\n",
    "\n",
    "\n",
    "\tf1_weighted_per_fold.clear()\n",
    "\tf1_macro_per_fold.clear()\n",
    "\tf1_micro_per_fold.clear()\n",
    "\ttestacc_per_fold.clear()\n",
    "\tprecision_per_fold.clear()\n",
    "\trecall_per_fold.clear()\n",
    "\n",
    "\tf1_weighted_per_level.clear()\n",
    "\tf1_macro_per_level.clear()\n",
    "\tf1_micro_per_level.clear()\n",
    "\ttestacc_per_level.clear()\n",
    "\tprecision_per_level.clear()\n",
    "\trecall_per_level.clear()\n",
    "\n",
    "\tkfold1 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\tkfold2 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\tfold = 1\n",
    "\n",
    "\tfor train_idx_1, test_idx_1 in kfold2.split(X.T, y):\n",
    "\t\tX_train, X_test = X.T[train_idx_1], X.T[test_idx_1]\n",
    "\t\tX_train, X_test = X_train[..., None], X_test[..., None]\n",
    "\t\ty_train, y_test = y[train_idx_1], y[test_idx_1]\n",
    "\t\tf1_weighted_per_fold.clear()\n",
    "\t\tf1_macro_per_fold.clear()\n",
    "\t\tf1_micro_per_fold.clear()\n",
    "\t\ttestacc_per_fold.clear()\n",
    "\t\tprecision_per_fold.clear()\n",
    "\t\trecall_per_fold.clear()\n",
    "\t\tfor train_ix, test_ix in kfold1.split(X_train, y_train):\n",
    "\t\t\ttrain_X, test_X = X_train[train_ix], X_train[test_ix]\n",
    "\t\t\ttrain_y, test_y = y_train[train_ix], y_train[test_ix]\n",
    "\n",
    "\t\t\tN = X_train.shape[-2]      # Number of nodes in the graphs\n",
    "\t\t\tF = X_train.shape[-1]      # Node features dimensionality\n",
    "\n",
    "\t\t\t# Model definition\n",
    "\t\t\tX_in = Input(shape=(N, F))\n",
    "\t\t\tA_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "\t\t\t# dropout_1 = Dropout(dropout)(X_in)\n",
    "\t\t\tbn_1 = BatchNormalization()(X_in)\n",
    "\t\t\tgraph_conv_1 = ChebConv(32,\n",
    "\t\t\t\t\t\t\t\t\tK=K,\n",
    "\t\t\t\t\t\t\t\t\tactivation='relu',\n",
    "\t\t\t\t\t\t\t\t\tkernel_regularizer=l2(l2_reg),\n",
    "\t\t\t\t\t\t\t\t\tuse_bias=False)([bn_1, A_in])\n",
    "\t\t\t# dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "\t\t\tbn_2 = BatchNormalization()(graph_conv_1)\n",
    "\t\t\tgraph_conv_2 = ChebConv(32,\n",
    "\t\t\t\t\t\t\t\t\tK=K,\n",
    "\t\t\t\t\t\t\t\t\tactivation='relu',\n",
    "\t\t\t\t\t\t\t\t\tuse_bias=False)([bn_2, A_in])\n",
    "\t\t\tflatten = Flatten()(graph_conv_2)\n",
    "\t\t\tfc_1 = Dense(64, activation='relu')(flatten)\n",
    "\t\t\tdropout_1 = Dropout(0.3, seed=42)(fc_1)\n",
    "\t\t\tfc_2 = Dense(32, activation='relu')(dropout_1)\n",
    "\t\t\toutput = Dense(n_out, activation='softmax')(fc_2)\n",
    "\n",
    "\t\t\t# Build model\n",
    "\t\t\tmodel = Model(inputs=[X_in, A_in], outputs=output)\n",
    "\t\t\toptimizer = Adam(lr=learning_rate)\n",
    "\t\t\tmodel.compile(optimizer=optimizer,\n",
    "\t\t\t\t\t\t  loss='sparse_categorical_crossentropy',\n",
    "\t\t\t\t\t\t  metrics=['acc'])\n",
    "\n",
    "\n",
    "\t\t\t# Train model\n",
    "\t\t\tvalidation_data = (test_X, test_y)\n",
    "\t\t\tmodel.fit(train_X,\n",
    "\t\t\t\t\t  train_y,\n",
    "\t\t\t\t\t  batch_size=16,\n",
    "\t\t\t\t\t  validation_data=validation_data,\n",
    "\t\t\t\t\t  epochs=10, verbose=0)\n",
    "\n",
    "\t\t\ty_pred = model.predict(X_test, verbose=1)\n",
    "\t\t\ty_p = []\n",
    "\t\t\tfor row in y_pred:\n",
    "\t\t\t\ty_p.append(np.argmax(row))\n",
    "\t\t\ttarget_names = ['0', '1', '2']\n",
    "\t\t\tprint(\"Fold: \", fold)\n",
    "\t\t\tfold += 1\n",
    "\t\t\t# print(classification_report(y_test, y_p, target_names=target_names))\n",
    "\t\t\tf1_weighted_per_fold.append(f1_score(y_test, y_p, average='weighted'))\n",
    "\t\t\tf1_macro_per_fold.append(f1_score(y_test, y_p, average='macro'))\n",
    "\t\t\tf1_micro_per_fold.append(f1_score(y_test, y_p, average='micro'))\n",
    "\t\t\ttestacc_per_fold.append(accuracy_score(y_test, y_p))\n",
    "\t\t\tprecision_per_fold.append(precision_score(y_test, y_p,  average='micro'))\n",
    "\t\t\trecall_per_fold.append(recall_score(y_test, y_p,  average='micro'))\n",
    "\t\t\t\n",
    "\t\tf1_weighted_per_level.append(np.mean(f1_weighted_per_fold))\n",
    "\t\tf1_macro_per_level.append(np.mean(f1_macro_per_fold))\n",
    "\t\tf1_micro_per_level.append(np.mean(f1_micro_per_fold))\n",
    "\t\ttestacc_per_level.append(np.mean(testacc_per_fold))\n",
    "\t\tprecision_per_level.append(np.mean(precision_per_fold))\n",
    "\t\trecall_per_level.append(np.mean(recall_per_fold))\n",
    "\n",
    "\t# APPEND METRICS\n",
    "\tscores = [file, np.mean(f1_weighted_per_level), np.mean(f1_macro_per_level), np.mean(f1_micro_per_level), np.mean(testacc_per_level), np.mean(precision_per_level), np.mean(recall_per_level)]\n",
    "\tprint(scores)\n",
    "# \tmetric_scores_per_pathway.append(scores)\n",
    "\n",
    "# \t# GENERATE OUTPUT CSV\n",
    "# \tfull_data = X.T\n",
    "# \tfull_data = full_data[..., None]\n",
    "# \tgcn_pathway_output = model.predict(full_data)\n",
    "# \tfilename_output_csv = os.path.join(\"/Users/ishitamed/Desktop/IIITH/Data/GCN_pathway_output_csv\",file)\n",
    "# \tnp.savetxt(filename_output_csv,gcn_pathway_output)\n",
    "\n",
    "# \t# REMOVE GARBAGE\n",
    "# \tdel pathway\n",
    "# \tdel X_train\n",
    "# \tdel X_test\n",
    "# \tdel train_X\n",
    "# \tdel test_X\n",
    "# \tdel A\n",
    "# \tdel adjacency_matrix\n",
    "# \tdel X\n",
    "# \tdel y\n",
    "\n",
    "# # SAVE METRICS FOR ALL PATHWAYS\n",
    "# metric_scores_df = pd.DataFrame(metric_scores_per_pathway, index=[\"Name\", \"f1-weighted\", \"f1-macro\", \"f1-micro\", \"test-acc\", \"prec\", \"recall\"])\n",
    "# metric_scores_df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pathways_not_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=os.listdir(kegg_pathways_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hsa01522 .csv',\n",
       " 'hsa03320 .csv',\n",
       " 'hsa04010 .csv',\n",
       " 'hsa04012 .csv',\n",
       " 'hsa04014 .csv',\n",
       " 'hsa04015 .csv',\n",
       " 'hsa04020 .csv',\n",
       " 'hsa04022 .csv',\n",
       " 'hsa04024 .csv',\n",
       " 'hsa04062 .csv',\n",
       " 'hsa04064 .csv',\n",
       " 'hsa04066 .csv',\n",
       " 'hsa04068 .csv',\n",
       " 'hsa04071 .csv',\n",
       " 'hsa04072 .csv',\n",
       " 'hsa04110 .csv',\n",
       " 'hsa04115 .csv',\n",
       " 'hsa04137 .csv',\n",
       " 'hsa04140 .csv',\n",
       " 'hsa04142 .csv',\n",
       " 'hsa04144 .csv',\n",
       " 'hsa04145 .csv',\n",
       " 'hsa04150 .csv',\n",
       " 'hsa04151 .csv',\n",
       " 'hsa04152 .csv',\n",
       " 'hsa04210 .csv',\n",
       " 'hsa04211 .csv',\n",
       " 'hsa04216 .csv',\n",
       " 'hsa04217 .csv',\n",
       " 'hsa04218 .csv',\n",
       " 'hsa04261 .csv',\n",
       " 'hsa04270 .csv',\n",
       " 'hsa04310 .csv',\n",
       " 'hsa04340 .csv',\n",
       " 'hsa04350 .csv',\n",
       " 'hsa04360 .csv',\n",
       " 'hsa04370 .csv',\n",
       " 'hsa04371 .csv',\n",
       " 'hsa04380 .csv',\n",
       " 'hsa04390 .csv',\n",
       " 'hsa04510 .csv',\n",
       " 'hsa04520 .csv',\n",
       " 'hsa04530 .csv',\n",
       " 'hsa04550 .csv',\n",
       " 'hsa04610 .csv',\n",
       " 'hsa04611 .csv',\n",
       " 'hsa04614 .csv',\n",
       " 'hsa04620 .csv',\n",
       " 'hsa04621 .csv',\n",
       " 'hsa04622 .csv',\n",
       " 'hsa04623 .csv',\n",
       " 'hsa04625 .csv',\n",
       " 'hsa04630 .csv',\n",
       " 'hsa04650 .csv',\n",
       " 'hsa04657 .csv',\n",
       " 'hsa04658 .csv',\n",
       " 'hsa04659 .csv',\n",
       " 'hsa04660 .csv',\n",
       " 'hsa04662 .csv',\n",
       " 'hsa04664 .csv',\n",
       " 'hsa04666 .csv',\n",
       " 'hsa04668 .csv',\n",
       " 'hsa04670 .csv',\n",
       " 'hsa04713 .csv',\n",
       " 'hsa04714 .csv',\n",
       " 'hsa04720 .csv',\n",
       " 'hsa04722 .csv',\n",
       " 'hsa04723 .csv',\n",
       " 'hsa04724 .csv',\n",
       " 'hsa04725 .csv',\n",
       " 'hsa04726 .csv',\n",
       " 'hsa04728 .csv',\n",
       " 'hsa04730 .csv',\n",
       " 'hsa04750 .csv',\n",
       " 'hsa04810 .csv',\n",
       " 'hsa04910 .csv',\n",
       " 'hsa04911 .csv',\n",
       " 'hsa04912 .csv',\n",
       " 'hsa04913 .csv',\n",
       " 'hsa04915 .csv',\n",
       " 'hsa04916 .csv',\n",
       " 'hsa04917 .csv',\n",
       " 'hsa04918 .csv',\n",
       " 'hsa04919 .csv',\n",
       " 'hsa04920 .csv',\n",
       " 'hsa04921 .csv',\n",
       " 'hsa04922 .csv',\n",
       " 'hsa04923 .csv',\n",
       " 'hsa04924 .csv',\n",
       " 'hsa04925 .csv',\n",
       " 'hsa04926 .csv',\n",
       " 'hsa04927 .csv',\n",
       " 'hsa04928 .csv',\n",
       " 'hsa04929 .csv',\n",
       " 'hsa04935 .csv',\n",
       " 'hsa04960 .csv',\n",
       " 'hsa04961 .csv',\n",
       " 'hsa04962 .csv',\n",
       " 'hsa04970 .csv',\n",
       " 'hsa04971 .csv',\n",
       " 'hsa04972 .csv',\n",
       " 'hsa04973 .csv',\n",
       " 'hsa04976 .csv',\n",
       " 'hsa04977 .csv',\n",
       " 'hsa04978 .csv',\n",
       " 'hsa04979 .csv',\n",
       " 'hsa05200 .csv',\n",
       " 'hsa05202 .csv',\n",
       " 'hsa05204 .csv',\n",
       " 'hsa05205 .csv',\n",
       " 'hsa05211 .csv',\n",
       " 'hsa05230 .csv',\n",
       " 'hsa05231 .csv',\n",
       " 'hsa05235 .csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
