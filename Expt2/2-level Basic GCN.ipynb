{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway = pd.read_csv(\"hsa05211_pathway.csv\")\n",
    "pathway.rename(columns={\"Unnamed: 0\": \"idx\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hsa:5781</td>\n",
       "      <td>hsa:5594</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>indirect effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>hsa:5781</td>\n",
       "      <td>hsa:5595</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>indirect effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>hsa:405</td>\n",
       "      <td>hsa:2034</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>binding/association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>hsa:405</td>\n",
       "      <td>hsa:3091</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>binding/association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hsa:9915</td>\n",
       "      <td>hsa:2034</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>binding/association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>109</td>\n",
       "      <td>hsa:5894</td>\n",
       "      <td>hsa:5605</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>phosphorylation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>hsa:673</td>\n",
       "      <td>hsa:5604</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>activation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>hsa:673</td>\n",
       "      <td>hsa:5604</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>phosphorylation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>112</td>\n",
       "      <td>hsa:673</td>\n",
       "      <td>hsa:5605</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>activation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>113</td>\n",
       "      <td>hsa:673</td>\n",
       "      <td>hsa:5605</td>\n",
       "      <td>PPrel</td>\n",
       "      <td>phosphorylation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx      from        to   type              subtype\n",
       "0      1  hsa:5781  hsa:5594  PPrel      indirect effect\n",
       "1      2  hsa:5781  hsa:5595  PPrel      indirect effect\n",
       "2      3   hsa:405  hsa:2034  PPrel  binding/association\n",
       "3      4   hsa:405  hsa:3091  PPrel  binding/association\n",
       "4      5  hsa:9915  hsa:2034  PPrel  binding/association\n",
       "..   ...       ...       ...    ...                  ...\n",
       "108  109  hsa:5894  hsa:5605  PPrel      phosphorylation\n",
       "109  110   hsa:673  hsa:5604  PPrel           activation\n",
       "110  111   hsa:673  hsa:5604  PPrel      phosphorylation\n",
       "111  112   hsa:673  hsa:5605  PPrel           activation\n",
       "112  113   hsa:673  hsa:5605  PPrel      phosphorylation\n",
       "\n",
       "[113 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Log_transformed_889_RCC.csv',\n",
       " 'KIRP_290_tumors_log_transformed.csv',\n",
       " '.DS_Store',\n",
       " 'Clinical_KICH_81_tumors.csv',\n",
       " 'KIRC_518_tumors_log_transformed.csv',\n",
       " 'KICH_81_tumors_log_transformed.csv',\n",
       " 'Log_transformed_25_KICH_normal.csv',\n",
       " 'Clinical_KIRP_290_tumors.csv',\n",
       " 'Log_transformed_129_normal.csv',\n",
       " 'Clinical_KIRC_518_tumors.csv',\n",
       " 'Log_transformed_72_KIRC_normal.csv',\n",
       " 'Log_transformed_32_KIRP_normal.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "path = '/Users/ishitamed/Downloads/GCN_Dataset/CSV/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, filename, transpose=True):\n",
    "    '''\n",
    "        Loads the dataset and converts into its transpose with appropriate columns\n",
    "    '''\n",
    "    df = pd.read_csv(os.path.join(path, filename))\n",
    "    df.rename(columns={\"Unnamed: 0\": \"pid\"}, inplace=True)\n",
    "    if transpose:\n",
    "        df = df.astype({\"pid\": str})\n",
    "        df = df.T\n",
    "        new_header = df.iloc[0] \n",
    "        df = df[1:]\n",
    "        df.columns = new_header\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kirp = load_dataset(path,'KIRP_290_tumors_log_transformed.csv',transpose=True)\n",
    "df_kirc = load_dataset(path,'KIRC_518_tumors_log_transformed.csv',transpose=True)\n",
    "df_kich = load_dataset(path,'KICH_81_tumors_log_transformed.csv',transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kirp['y'] = 0\n",
    "df_kirc['y'] = 1\n",
    "df_kich['y'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_kirp, df_kirc, df_kich])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pid</th>\n",
       "      <th>100130426</th>\n",
       "      <th>100133144</th>\n",
       "      <th>100134869</th>\n",
       "      <th>10357</th>\n",
       "      <th>10431</th>\n",
       "      <th>136542</th>\n",
       "      <th>155060</th>\n",
       "      <th>26823</th>\n",
       "      <th>280660</th>\n",
       "      <th>317712</th>\n",
       "      <th>...</th>\n",
       "      <th>11130</th>\n",
       "      <th>7789</th>\n",
       "      <th>158586</th>\n",
       "      <th>79364</th>\n",
       "      <th>440590</th>\n",
       "      <th>79699</th>\n",
       "      <th>7791</th>\n",
       "      <th>23140</th>\n",
       "      <th>26009</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2K-A9WE</th>\n",
       "      <td>0</td>\n",
       "      <td>2.59679</td>\n",
       "      <td>2.75228</td>\n",
       "      <td>5.79911</td>\n",
       "      <td>9.61253</td>\n",
       "      <td>0</td>\n",
       "      <td>8.20371</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.55048</td>\n",
       "      <td>4.8189</td>\n",
       "      <td>7.50224</td>\n",
       "      <td>10.2856</td>\n",
       "      <td>1.61103</td>\n",
       "      <td>9.57221</td>\n",
       "      <td>13.1108</td>\n",
       "      <td>10.5605</td>\n",
       "      <td>9.93522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2Z-A9J1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.17612</td>\n",
       "      <td>3.67643</td>\n",
       "      <td>6.19717</td>\n",
       "      <td>10.1783</td>\n",
       "      <td>0</td>\n",
       "      <td>7.66327</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6057</td>\n",
       "      <td>4.91841</td>\n",
       "      <td>7.43853</td>\n",
       "      <td>9.92169</td>\n",
       "      <td>1.31429</td>\n",
       "      <td>9.00269</td>\n",
       "      <td>12.6</td>\n",
       "      <td>10.3448</td>\n",
       "      <td>8.4931</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2Z-A9J2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.94931</td>\n",
       "      <td>2.11527</td>\n",
       "      <td>4.90228</td>\n",
       "      <td>10.3464</td>\n",
       "      <td>0</td>\n",
       "      <td>6.28195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.83897</td>\n",
       "      <td>4.75582</td>\n",
       "      <td>7.12681</td>\n",
       "      <td>9.74123</td>\n",
       "      <td>1.16279</td>\n",
       "      <td>8.65696</td>\n",
       "      <td>12.7855</td>\n",
       "      <td>9.63509</td>\n",
       "      <td>8.43738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2Z-A9J3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.76666</td>\n",
       "      <td>2.59753</td>\n",
       "      <td>6.24673</td>\n",
       "      <td>9.81382</td>\n",
       "      <td>0</td>\n",
       "      <td>9.38455</td>\n",
       "      <td>0.445621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.7991</td>\n",
       "      <td>5.00565</td>\n",
       "      <td>7.39412</td>\n",
       "      <td>10.3487</td>\n",
       "      <td>0</td>\n",
       "      <td>8.83842</td>\n",
       "      <td>13.0534</td>\n",
       "      <td>10.1111</td>\n",
       "      <td>9.30753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2Z-A9J5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.20777</td>\n",
       "      <td>3.05617</td>\n",
       "      <td>6.24278</td>\n",
       "      <td>10.2727</td>\n",
       "      <td>0</td>\n",
       "      <td>7.81579</td>\n",
       "      <td>0.835358</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.46504</td>\n",
       "      <td>3.67409</td>\n",
       "      <td>6.92818</td>\n",
       "      <td>9.72935</td>\n",
       "      <td>1.74541</td>\n",
       "      <td>8.81007</td>\n",
       "      <td>12.4673</td>\n",
       "      <td>10.1546</td>\n",
       "      <td>8.91459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-BP-4994</th>\n",
       "      <td>0</td>\n",
       "      <td>2.66505</td>\n",
       "      <td>3.66125</td>\n",
       "      <td>6.29367</td>\n",
       "      <td>9.42055</td>\n",
       "      <td>0</td>\n",
       "      <td>6.83102</td>\n",
       "      <td>1.37473</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.62133</td>\n",
       "      <td>7.26878</td>\n",
       "      <td>9.5991</td>\n",
       "      <td>10.89</td>\n",
       "      <td>1.04411</td>\n",
       "      <td>10.8615</td>\n",
       "      <td>10.6078</td>\n",
       "      <td>10.3058</td>\n",
       "      <td>9.62504</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-B0-4688</th>\n",
       "      <td>0</td>\n",
       "      <td>3.89335</td>\n",
       "      <td>3.6834</td>\n",
       "      <td>5.72368</td>\n",
       "      <td>9.12061</td>\n",
       "      <td>0</td>\n",
       "      <td>7.78298</td>\n",
       "      <td>1.50396</td>\n",
       "      <td>5.08968</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.82246</td>\n",
       "      <td>6.20389</td>\n",
       "      <td>8.91646</td>\n",
       "      <td>9.25762</td>\n",
       "      <td>0.79452</td>\n",
       "      <td>9.85502</td>\n",
       "      <td>12.2808</td>\n",
       "      <td>11.1702</td>\n",
       "      <td>9.92699</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-B0-4696</th>\n",
       "      <td>0</td>\n",
       "      <td>4.05197</td>\n",
       "      <td>4.57621</td>\n",
       "      <td>6.30959</td>\n",
       "      <td>9.38998</td>\n",
       "      <td>0</td>\n",
       "      <td>8.69391</td>\n",
       "      <td>0.597031</td>\n",
       "      <td>6.31145</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.1245</td>\n",
       "      <td>6.70077</td>\n",
       "      <td>9.13638</td>\n",
       "      <td>8.69569</td>\n",
       "      <td>1.01799</td>\n",
       "      <td>9.85832</td>\n",
       "      <td>12.7399</td>\n",
       "      <td>9.64492</td>\n",
       "      <td>10.0795</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-B0-4699</th>\n",
       "      <td>0</td>\n",
       "      <td>2.91608</td>\n",
       "      <td>3.30334</td>\n",
       "      <td>5.85481</td>\n",
       "      <td>9.95274</td>\n",
       "      <td>0</td>\n",
       "      <td>7.63986</td>\n",
       "      <td>1.47913</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.087</td>\n",
       "      <td>6.2496</td>\n",
       "      <td>8.97293</td>\n",
       "      <td>10.501</td>\n",
       "      <td>3.24673</td>\n",
       "      <td>10.6398</td>\n",
       "      <td>11.7414</td>\n",
       "      <td>9.99253</td>\n",
       "      <td>9.58994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-B0-4821</th>\n",
       "      <td>0</td>\n",
       "      <td>1.32487</td>\n",
       "      <td>1.79867</td>\n",
       "      <td>6.21885</td>\n",
       "      <td>9.94278</td>\n",
       "      <td>0</td>\n",
       "      <td>8.95393</td>\n",
       "      <td>1.21909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.49011</td>\n",
       "      <td>4.61245</td>\n",
       "      <td>7.83906</td>\n",
       "      <td>10.3602</td>\n",
       "      <td>5.67305</td>\n",
       "      <td>8.92794</td>\n",
       "      <td>12.1805</td>\n",
       "      <td>10.5162</td>\n",
       "      <td>8.85947</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows Ã— 20532 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "pid          100130426 100133144 100134869    10357    10431 136542   155060  \\\n",
       "TCGA-2K-A9WE         0   2.59679   2.75228  5.79911  9.61253      0  8.20371   \n",
       "TCGA-2Z-A9J1         0   3.17612   3.67643  6.19717  10.1783      0  7.66327   \n",
       "TCGA-2Z-A9J2         0   1.94931   2.11527  4.90228  10.3464      0  6.28195   \n",
       "TCGA-2Z-A9J3         0   2.76666   2.59753  6.24673  9.81382      0  9.38455   \n",
       "TCGA-2Z-A9J5         0   1.20777   3.05617  6.24278  10.2727      0  7.81579   \n",
       "...                ...       ...       ...      ...      ...    ...      ...   \n",
       "TCGA-BP-4994         0   2.66505   3.66125  6.29367  9.42055      0  6.83102   \n",
       "TCGA-B0-4688         0   3.89335    3.6834  5.72368  9.12061      0  7.78298   \n",
       "TCGA-B0-4696         0   4.05197   4.57621  6.30959  9.38998      0  8.69391   \n",
       "TCGA-B0-4699         0   2.91608   3.30334  5.85481  9.95274      0  7.63986   \n",
       "TCGA-B0-4821         0   1.32487   1.79867  6.21885  9.94278      0  8.95393   \n",
       "\n",
       "pid              26823   280660 317712  ...    11130     7789   158586  \\\n",
       "TCGA-2K-A9WE         0        0      0  ...  7.55048   4.8189  7.50224   \n",
       "TCGA-2Z-A9J1         0        0      0  ...   7.6057  4.91841  7.43853   \n",
       "TCGA-2Z-A9J2         0        0      0  ...  7.83897  4.75582  7.12681   \n",
       "TCGA-2Z-A9J3  0.445621        0      0  ...   8.7991  5.00565  7.39412   \n",
       "TCGA-2Z-A9J5  0.835358        0      0  ...  7.46504  3.67409  6.92818   \n",
       "...                ...      ...    ...  ...      ...      ...      ...   \n",
       "TCGA-BP-4994   1.37473        0      0  ...  6.62133  7.26878   9.5991   \n",
       "TCGA-B0-4688   1.50396  5.08968      0  ...  8.82246  6.20389  8.91646   \n",
       "TCGA-B0-4696  0.597031  6.31145      0  ...  10.1245  6.70077  9.13638   \n",
       "TCGA-B0-4699   1.47913        0      0  ...    9.087   6.2496  8.97293   \n",
       "TCGA-B0-4821   1.21909        0      0  ...  8.49011  4.61245  7.83906   \n",
       "\n",
       "pid             79364   440590    79699     7791    23140    26009  y  \n",
       "TCGA-2K-A9WE  10.2856  1.61103  9.57221  13.1108  10.5605  9.93522  0  \n",
       "TCGA-2Z-A9J1  9.92169  1.31429  9.00269     12.6  10.3448   8.4931  0  \n",
       "TCGA-2Z-A9J2  9.74123  1.16279  8.65696  12.7855  9.63509  8.43738  0  \n",
       "TCGA-2Z-A9J3  10.3487        0  8.83842  13.0534  10.1111  9.30753  0  \n",
       "TCGA-2Z-A9J5  9.72935  1.74541  8.81007  12.4673  10.1546  8.91459  0  \n",
       "...               ...      ...      ...      ...      ...      ... ..  \n",
       "TCGA-BP-4994    10.89  1.04411  10.8615  10.6078  10.3058  9.62504  2  \n",
       "TCGA-B0-4688  9.25762  0.79452  9.85502  12.2808  11.1702  9.92699  2  \n",
       "TCGA-B0-4696  8.69569  1.01799  9.85832  12.7399  9.64492  10.0795  2  \n",
       "TCGA-B0-4699   10.501  3.24673  10.6398  11.7414  9.99253  9.58994  2  \n",
       "TCGA-B0-4821  10.3602  5.67305  8.92794  12.1805  10.5162  8.85947  2  \n",
       "\n",
       "[889 rows x 20532 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_used = set()\n",
    "\n",
    "for i in range(len(pathway)):\n",
    "    genes_used.add(pathway.iloc[i]['from'][4:])\n",
    "    genes_used.add(pathway.iloc[i]['to'][4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for gene in genes_used:\n",
    "    if gene not in data.columns:\n",
    "        to_remove.append(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in to_remove:\n",
    "    genes_used.remove(gene)\n",
    "genes_used = list(genes_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in to_remove:\n",
    "    pathway = pathway[pathway['from']!=(\"hsa:\"+str(gene))]\n",
    "    pathway = pathway[pathway['to']!=(\"hsa:\"+str(gene))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = len(genes_used)\n",
    "edges = len(pathway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 111)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_used.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to map gene_id to node_number\n",
    "node_map = {}\n",
    "count = 0\n",
    "for gene in genes_used:\n",
    "    node_map[(\"hsa:\"+str(gene))] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hsa:10000': 0,\n",
       " 'hsa:10298': 1,\n",
       " 'hsa:112398': 2,\n",
       " 'hsa:112399': 3,\n",
       " 'hsa:1387': 4,\n",
       " 'hsa:1398': 5,\n",
       " 'hsa:1399': 6,\n",
       " 'hsa:2033': 7,\n",
       " 'hsa:2034': 8,\n",
       " 'hsa:207': 9,\n",
       " 'hsa:208': 10,\n",
       " 'hsa:2113': 11,\n",
       " 'hsa:2549': 12,\n",
       " 'hsa:2885': 13,\n",
       " 'hsa:2889': 14,\n",
       " 'hsa:3082': 15,\n",
       " 'hsa:3091': 16,\n",
       " 'hsa:369': 17,\n",
       " 'hsa:3725': 18,\n",
       " 'hsa:405': 19,\n",
       " 'hsa:4233': 20,\n",
       " 'hsa:5058': 21,\n",
       " 'hsa:5062': 22,\n",
       " 'hsa:5063': 23,\n",
       " 'hsa:5155': 24,\n",
       " 'hsa:5290': 25,\n",
       " 'hsa:5291': 26,\n",
       " 'hsa:5293': 27,\n",
       " 'hsa:5295': 28,\n",
       " 'hsa:5296': 29,\n",
       " 'hsa:54583': 30,\n",
       " 'hsa:5594': 31,\n",
       " 'hsa:5595': 32,\n",
       " 'hsa:5604': 33,\n",
       " 'hsa:5605': 34,\n",
       " 'hsa:56924': 35,\n",
       " 'hsa:57144': 36,\n",
       " 'hsa:572': 37,\n",
       " 'hsa:5781': 38,\n",
       " 'hsa:5879': 39,\n",
       " 'hsa:5894': 40,\n",
       " 'hsa:5906': 41,\n",
       " 'hsa:5908': 42,\n",
       " 'hsa:6513': 43,\n",
       " 'hsa:6654': 44,\n",
       " 'hsa:6655': 45,\n",
       " 'hsa:673': 46,\n",
       " 'hsa:7039': 47,\n",
       " 'hsa:7040': 48,\n",
       " 'hsa:7042': 49,\n",
       " 'hsa:7043': 50,\n",
       " 'hsa:7422': 51,\n",
       " 'hsa:8503': 52,\n",
       " 'hsa:9915': 53,\n",
       " 'hsa:998': 54}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.zeros((nodes,nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(edges):\n",
    "    n1 = pathway.iloc[i]['from']\n",
    "    n2 = pathway.iloc[i]['to']\n",
    "    n1 = node_map[n1]\n",
    "    n2 = node_map[n2]\n",
    "    adjacency_matrix[n1][n2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 55)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "A = sparse.csr_matrix(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create node features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[genes_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.15260427895225, 8.73043277438035, 8.04926046332519, ...,\n",
       "        9.45358753933941, 10.0572065531123, 8.55607298957492],\n",
       "       [10.2330444012656, 11.3456325558529, 10.3532240318824, ...,\n",
       "        9.329202099016449, 10.5715251309464, 11.3560062175436],\n",
       "       [10.554253079053401, 10.6850991720066, 10.8393061591129, ...,\n",
       "        9.958333757156698, 10.470089410577, 11.1719398603242],\n",
       "       ...,\n",
       "       [8.83650803033427, 8.78656369179624, 9.51807478161928, ...,\n",
       "        10.3666719161118, 10.5155815797454, 8.922688457336019],\n",
       "       [9.47662875872076, 9.09187434314344, 7.7751229905654595, ...,\n",
       "        4.815862299921401, 11.212210614512301, 8.29448290520082],\n",
       "       [12.181189535325599, 12.371904575773002, 12.1992827891219, ...,\n",
       "        13.265352794367699, 12.543851080924302, 11.408165185591098]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 889)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create target label matrix\n",
    "\n",
    "NOT NEEDED THO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHot = False\n",
    "\n",
    "if OneHot:\n",
    "    y = []\n",
    "    for i in data['y']:\n",
    "        if i==0:\n",
    "            y.append([1,0,0])\n",
    "        elif i==1:\n",
    "            y.append([0,1,0])\n",
    "        elif i==2:\n",
    "            y.append([0,0,1])\n",
    "else:\n",
    "    y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data['y'].to_csv(\"patient_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from spektral.layers import GraphConv\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from spektral.layers import ChebConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "l2_reg = 5e-4         # Regularization rate for l2\n",
    "learning_rate = 5e-4  # Learning rate for SGD\n",
    "batch_size = 32       # Batch size\n",
    "epochs = 50         # Number of training epochs\n",
    "es_patience = 0      # Patience fot early stopping\n",
    "channels = 16           # Number of channels in the first layer\n",
    "K = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train[..., None], X_test[..., None]\n",
    "# N = X_train.shape[-2]      # Number of nodes in the graphs\n",
    "# F = X_train.shape[-1]      # Node features dimensionality\n",
    "n_out = 3                  # Dimension of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spektral/utils/convolution.py:30: RuntimeWarning: divide by zero encountered in power\n",
      "  degrees = np.power(np.array(A.sum(1)), k).flatten()\n"
     ]
    }
   ],
   "source": [
    "fltr = ChebConv.preprocess(A).astype('f4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 55)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fltr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model definition\n",
    "# X_in = Input(shape=(N, F))\n",
    "# A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "# # dropout_1 = Dropout(dropout)(X_in)\n",
    "# graph_conv_1 = ChebConv(16,\n",
    "#                         K=K,\n",
    "#                         activation='relu',\n",
    "#                         kernel_regularizer=l2(l2_reg),\n",
    "#                         use_bias=False)([X_in, A_in])\n",
    "# # dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "# graph_conv_2 = ChebConv(16,\n",
    "#                         K=K,\n",
    "#                         activation='relu',\n",
    "#                         use_bias=False)([graph_conv_1, A_in])\n",
    "# flatten = Flatten()(graph_conv_2)\n",
    "# fc_1 = Dense(512, activation='relu')(flatten)\n",
    "# dropout_1 = Dropout(0.3)(fc_1)\n",
    "# fc_2 = Dense(128, activation='relu')(dropout_1)\n",
    "# output = Dense(n_out, activation='softmax')(fc_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Build model\n",
    "# model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "# optimizer = Adam(lr=learning_rate)\n",
    "# model.compile(optimizer=optimizer,\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['acc'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train model\n",
    "# validation_data = (X_test, y_test)\n",
    "# model.fit(X_train,\n",
    "#           y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           validation_data=validation_data,\n",
    "#           epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# y_true = y_test\n",
    "# y_pred = model.predict(X_test, verbose=1)\n",
    "# y_p = []\n",
    "# for row in y_pred:\n",
    "#     y_p.append(np.argmax(row))\n",
    "# target_names = ['kirp 0', 'kirc 1', 'kich 2']\n",
    "# print(classification_report(y_true, y_p, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted_per_fold = []\n",
    "f1_macro_per_fold = []\n",
    "f1_micro_per_fold = []\n",
    "testacc_per_fold = []\n",
    "precision_per_fold = []\n",
    "recall_per_fold = []\n",
    "\n",
    "f1_weighted_per_level = []\n",
    "f1_macro_per_level = []\n",
    "f1_micro_per_level = []\n",
    "testacc_per_level = []\n",
    "precision_per_level=[]\n",
    "recall_per_level=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_per_fold = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0609 16:03:27.825738 4729851328 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 0s 495us/sample - loss: 0.4680 - acc: 0.8136 - val_loss: 1.6266 - val_acc: 0.6393\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 271us/sample - loss: 0.2466 - acc: 0.9416 - val_loss: 1.9471 - val_acc: 0.0984\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 242us/sample - loss: 0.2141 - acc: 0.9379 - val_loss: 1.6930 - val_acc: 0.1148\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 276us/sample - loss: 0.1784 - acc: 0.9586 - val_loss: 0.8854 - val_acc: 0.6230\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 228us/sample - loss: 0.1504 - acc: 0.9529 - val_loss: 0.7516 - val_acc: 0.6230\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 0s 227us/sample - loss: 0.1739 - acc: 0.9473 - val_loss: 0.6830 - val_acc: 0.6721\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 226us/sample - loss: 0.1537 - acc: 0.9510 - val_loss: 0.4621 - val_acc: 0.8852\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 227us/sample - loss: 0.1452 - acc: 0.9548 - val_loss: 0.4026 - val_acc: 0.8525\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 225us/sample - loss: 0.1144 - acc: 0.9680 - val_loss: 0.3219 - val_acc: 0.8852\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 320us/sample - loss: 0.1432 - acc: 0.9623 - val_loss: 0.2888 - val_acc: 0.9180\n",
      "Fold:  1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.76      0.96      0.85        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.89      0.95      0.91       297\n",
      "weighted avg       0.94      0.94      0.94       297\n",
      "\n",
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 0s 754us/sample - loss: 0.5138 - acc: 0.7910 - val_loss: 1.7384 - val_acc: 0.6230\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 342us/sample - loss: 0.2319 - acc: 0.9247 - val_loss: 1.7256 - val_acc: 0.0984\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 411us/sample - loss: 0.1872 - acc: 0.9416 - val_loss: 1.2871 - val_acc: 0.5902\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 411us/sample - loss: 0.1784 - acc: 0.9454 - val_loss: 1.0498 - val_acc: 0.6230\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 1s 2ms/sample - loss: 0.1576 - acc: 0.9586 - val_loss: 0.9137 - val_acc: 0.6230\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 1s 1ms/sample - loss: 0.1374 - acc: 0.9623 - val_loss: 0.7399 - val_acc: 0.6557\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 1s 1ms/sample - loss: 0.1298 - acc: 0.9680 - val_loss: 0.5127 - val_acc: 0.8197\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 851us/sample - loss: 0.1380 - acc: 0.9605 - val_loss: 0.4170 - val_acc: 0.8852\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 770us/sample - loss: 0.1149 - acc: 0.9680 - val_loss: 0.3266 - val_acc: 0.9016\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 389us/sample - loss: 0.1268 - acc: 0.9661 - val_loss: 0.3490 - val_acc: 0.8361\n",
      "Fold:  2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89        97\n",
      "           1       0.94      0.97      0.95       173\n",
      "           2       0.63      0.96      0.76        27\n",
      "\n",
      "    accuracy                           0.91       297\n",
      "   macro avg       0.85      0.91      0.87       297\n",
      "weighted avg       0.93      0.91      0.91       297\n",
      "\n",
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 0s 706us/sample - loss: 0.6978 - acc: 0.7288 - val_loss: 1.6520 - val_acc: 0.6557\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 243us/sample - loss: 0.3329 - acc: 0.8908 - val_loss: 1.6874 - val_acc: 0.5574\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 284us/sample - loss: 0.2407 - acc: 0.9228 - val_loss: 1.2587 - val_acc: 0.6393\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 250us/sample - loss: 0.1919 - acc: 0.9322 - val_loss: 1.1468 - val_acc: 0.4754\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 480us/sample - loss: 0.2046 - acc: 0.9360 - val_loss: 0.6405 - val_acc: 0.6721\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 0s 489us/sample - loss: 0.2004 - acc: 0.9548 - val_loss: 0.3864 - val_acc: 0.9016\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 474us/sample - loss: 0.1835 - acc: 0.9548 - val_loss: 0.4224 - val_acc: 0.8525\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 345us/sample - loss: 0.1577 - acc: 0.9473 - val_loss: 0.2610 - val_acc: 0.9344\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 465us/sample - loss: 0.1446 - acc: 0.9529 - val_loss: 0.1584 - val_acc: 0.9508\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 330us/sample - loss: 0.1632 - acc: 0.9510 - val_loss: 0.1557 - val_acc: 0.9836\n",
      "Fold:  3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93        97\n",
      "           1       0.97      0.97      0.97       173\n",
      "           2       0.81      0.96      0.88        27\n",
      "\n",
      "    accuracy                           0.95       297\n",
      "   macro avg       0.91      0.95      0.93       297\n",
      "weighted avg       0.95      0.95      0.95       297\n",
      "\n",
      "Train on 532 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "532/532 [==============================] - 1s 1ms/sample - loss: 0.4895 - acc: 0.8026 - val_loss: 1.5254 - val_acc: 0.5833\n",
      "Epoch 2/10\n",
      "532/532 [==============================] - 1s 2ms/sample - loss: 0.2371 - acc: 0.9248 - val_loss: 1.4364 - val_acc: 0.6833\n",
      "Epoch 3/10\n",
      "532/532 [==============================] - 0s 383us/sample - loss: 0.2192 - acc: 0.9342 - val_loss: 1.1259 - val_acc: 0.5333\n",
      "Epoch 4/10\n",
      "532/532 [==============================] - 0s 349us/sample - loss: 0.1992 - acc: 0.9455 - val_loss: 1.0579 - val_acc: 0.5667\n",
      "Epoch 5/10\n",
      "532/532 [==============================] - 0s 372us/sample - loss: 0.2031 - acc: 0.9474 - val_loss: 0.5877 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "532/532 [==============================] - 0s 285us/sample - loss: 0.1866 - acc: 0.9323 - val_loss: 0.4516 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "532/532 [==============================] - 0s 506us/sample - loss: 0.1846 - acc: 0.9436 - val_loss: 0.3194 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "532/532 [==============================] - 0s 399us/sample - loss: 0.1579 - acc: 0.9474 - val_loss: 0.2424 - val_acc: 0.9833\n",
      "Epoch 9/10\n",
      "532/532 [==============================] - 0s 245us/sample - loss: 0.1429 - acc: 0.9568 - val_loss: 0.1592 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "532/532 [==============================] - 0s 280us/sample - loss: 0.1343 - acc: 0.9530 - val_loss: 0.1137 - val_acc: 1.0000\n",
      "Fold:  4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.84      0.96      0.90        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.91      0.95      0.93       297\n",
      "weighted avg       0.95      0.94      0.94       297\n",
      "\n",
      "Train on 533 samples, validate on 59 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 1s 2ms/sample - loss: 0.5592 - acc: 0.7711 - val_loss: 0.8775 - val_acc: 0.5932\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 784us/sample - loss: 0.2557 - acc: 0.9306 - val_loss: 1.0713 - val_acc: 0.6441\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 0s 242us/sample - loss: 0.2209 - acc: 0.9268 - val_loss: 0.8571 - val_acc: 0.6441\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 375us/sample - loss: 0.1622 - acc: 0.9512 - val_loss: 0.8095 - val_acc: 0.6610\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 0s 869us/sample - loss: 0.1754 - acc: 0.9531 - val_loss: 0.7789 - val_acc: 0.6610\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 442us/sample - loss: 0.1433 - acc: 0.9625 - val_loss: 0.5854 - val_acc: 0.7966\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 0s 358us/sample - loss: 0.1394 - acc: 0.9568 - val_loss: 0.4837 - val_acc: 0.8644\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 0s 242us/sample - loss: 0.1419 - acc: 0.9550 - val_loss: 0.3614 - val_acc: 0.8814\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 0s 297us/sample - loss: 0.1060 - acc: 0.9644 - val_loss: 0.3715 - val_acc: 0.8644\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 0s 361us/sample - loss: 0.1124 - acc: 0.9700 - val_loss: 0.3586 - val_acc: 0.8814\n",
      "Fold:  5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        97\n",
      "           1       0.98      0.95      0.96       173\n",
      "           2       0.75      1.00      0.86        27\n",
      "\n",
      "    accuracy                           0.95       297\n",
      "   macro avg       0.90      0.96      0.92       297\n",
      "weighted avg       0.96      0.95      0.95       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 2ms/sample - loss: 0.4309 - acc: 0.8352 - val_loss: 1.4286 - val_acc: 0.6379\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 449us/sample - loss: 0.2349 - acc: 0.9232 - val_loss: 1.1434 - val_acc: 0.6379\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 320us/sample - loss: 0.2079 - acc: 0.9419 - val_loss: 1.3747 - val_acc: 0.6034\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 258us/sample - loss: 0.1884 - acc: 0.9532 - val_loss: 0.8805 - val_acc: 0.6379\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 363us/sample - loss: 0.1725 - acc: 0.9532 - val_loss: 0.7114 - val_acc: 0.6207\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 323us/sample - loss: 0.1503 - acc: 0.9607 - val_loss: 0.4895 - val_acc: 0.7759\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 264us/sample - loss: 0.1595 - acc: 0.9682 - val_loss: 0.3237 - val_acc: 0.9483\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 266us/sample - loss: 0.1325 - acc: 0.9625 - val_loss: 0.2727 - val_acc: 0.9483\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 281us/sample - loss: 0.1401 - acc: 0.9607 - val_loss: 0.1936 - val_acc: 0.9483\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 338us/sample - loss: 0.1368 - acc: 0.9625 - val_loss: 0.1870 - val_acc: 0.9655\n",
      "Fold:  6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.84      0.96      0.90        27\n",
      "\n",
      "    accuracy                           0.95       297\n",
      "   macro avg       0.92      0.95      0.93       297\n",
      "weighted avg       0.95      0.95      0.95       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 1ms/sample - loss: 0.4352 - acc: 0.8352 - val_loss: 1.5052 - val_acc: 0.5172\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 341us/sample - loss: 0.2298 - acc: 0.9176 - val_loss: 1.3781 - val_acc: 0.6552\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 493us/sample - loss: 0.1966 - acc: 0.9476 - val_loss: 1.3028 - val_acc: 0.5862\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 565us/sample - loss: 0.1825 - acc: 0.9513 - val_loss: 0.9527 - val_acc: 0.6552\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 504us/sample - loss: 0.1555 - acc: 0.9588 - val_loss: 0.8576 - val_acc: 0.6552\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 389us/sample - loss: 0.1415 - acc: 0.9532 - val_loss: 0.6055 - val_acc: 0.7069\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 283us/sample - loss: 0.1446 - acc: 0.9588 - val_loss: 0.4580 - val_acc: 0.8103\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 605us/sample - loss: 0.1338 - acc: 0.9607 - val_loss: 0.2857 - val_acc: 0.9310\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 256us/sample - loss: 0.1222 - acc: 0.9644 - val_loss: 0.2316 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 334us/sample - loss: 0.1237 - acc: 0.9625 - val_loss: 0.1819 - val_acc: 0.9483\n",
      "Fold:  7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.72      0.96      0.83        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.88      0.94      0.91       297\n",
      "weighted avg       0.95      0.94      0.94       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 2ms/sample - loss: 0.4891 - acc: 0.7959 - val_loss: 1.7374 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 538us/sample - loss: 0.2716 - acc: 0.9045 - val_loss: 1.6816 - val_acc: 0.6724\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 633us/sample - loss: 0.1816 - acc: 0.9494 - val_loss: 1.4850 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 399us/sample - loss: 0.1771 - acc: 0.9419 - val_loss: 1.1284 - val_acc: 0.6724\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 289us/sample - loss: 0.1684 - acc: 0.9532 - val_loss: 1.0348 - val_acc: 0.6034\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 434us/sample - loss: 0.1698 - acc: 0.9419 - val_loss: 0.6220 - val_acc: 0.7414\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 265us/sample - loss: 0.1409 - acc: 0.9588 - val_loss: 0.3393 - val_acc: 0.9310\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 261us/sample - loss: 0.1462 - acc: 0.9532 - val_loss: 0.2966 - val_acc: 0.9483\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 290us/sample - loss: 0.1142 - acc: 0.9625 - val_loss: 0.2631 - val_acc: 0.9483\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 266us/sample - loss: 0.1438 - acc: 0.9588 - val_loss: 0.1935 - val_acc: 0.9655\n",
      "Fold:  8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        97\n",
      "           1       0.99      0.94      0.96       173\n",
      "           2       0.76      0.96      0.85        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.89      0.95      0.92       297\n",
      "weighted avg       0.95      0.94      0.94       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 2ms/sample - loss: 0.4713 - acc: 0.8165 - val_loss: 1.1879 - val_acc: 0.6724\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 450us/sample - loss: 0.2583 - acc: 0.9082 - val_loss: 1.3685 - val_acc: 0.6379\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 722us/sample - loss: 0.2003 - acc: 0.9363 - val_loss: 1.1375 - val_acc: 0.6207\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 532us/sample - loss: 0.2107 - acc: 0.9363 - val_loss: 0.9597 - val_acc: 0.5517\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 474us/sample - loss: 0.1714 - acc: 0.9476 - val_loss: 0.6410 - val_acc: 0.6552\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 722us/sample - loss: 0.1600 - acc: 0.9588 - val_loss: 0.5115 - val_acc: 0.7586\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 699us/sample - loss: 0.1478 - acc: 0.9588 - val_loss: 0.4322 - val_acc: 0.8621\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 330us/sample - loss: 0.1362 - acc: 0.9607 - val_loss: 0.3841 - val_acc: 0.9138\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 343us/sample - loss: 0.1318 - acc: 0.9644 - val_loss: 0.2823 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 284us/sample - loss: 0.1318 - acc: 0.9588 - val_loss: 0.2153 - val_acc: 0.9655\n",
      "Fold:  9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.79      0.96      0.87        27\n",
      "\n",
      "    accuracy                           0.95       297\n",
      "   macro avg       0.90      0.95      0.92       297\n",
      "weighted avg       0.95      0.95      0.95       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 2ms/sample - loss: 0.4997 - acc: 0.8277 - val_loss: 1.5452 - val_acc: 0.5862\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 0s 586us/sample - loss: 0.2370 - acc: 0.9307 - val_loss: 1.3260 - val_acc: 0.5862\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 435us/sample - loss: 0.1950 - acc: 0.9476 - val_loss: 1.1481 - val_acc: 0.5862\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 413us/sample - loss: 0.1870 - acc: 0.9345 - val_loss: 1.0180 - val_acc: 0.6379\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 495us/sample - loss: 0.1860 - acc: 0.9476 - val_loss: 1.0928 - val_acc: 0.6207\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 366us/sample - loss: 0.1713 - acc: 0.9494 - val_loss: 0.9221 - val_acc: 0.6552\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 270us/sample - loss: 0.1582 - acc: 0.9532 - val_loss: 0.7163 - val_acc: 0.7241\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 843us/sample - loss: 0.1307 - acc: 0.9644 - val_loss: 0.4515 - val_acc: 0.8276\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 418us/sample - loss: 0.1352 - acc: 0.9682 - val_loss: 0.4317 - val_acc: 0.8276\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 672us/sample - loss: 0.1296 - acc: 0.9682 - val_loss: 0.3570 - val_acc: 0.8793\n",
      "Fold:  10\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.88        97\n",
      "           1       0.93      0.95      0.94       173\n",
      "           2       0.62      0.96      0.75        27\n",
      "\n",
      "    accuracy                           0.90       297\n",
      "   macro avg       0.85      0.90      0.86       297\n",
      "weighted avg       0.92      0.90      0.91       297\n",
      "\n",
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 1s 3ms/sample - loss: 0.5449 - acc: 0.7740 - val_loss: 1.1362 - val_acc: 0.6557\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 275us/sample - loss: 0.2590 - acc: 0.9247 - val_loss: 1.6214 - val_acc: 0.1148\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 255us/sample - loss: 0.1962 - acc: 0.9548 - val_loss: 1.1006 - val_acc: 0.5410\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 315us/sample - loss: 0.1701 - acc: 0.9492 - val_loss: 0.7308 - val_acc: 0.6557\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 362us/sample - loss: 0.1801 - acc: 0.9529 - val_loss: 0.4748 - val_acc: 0.9016\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 0s 478us/sample - loss: 0.1599 - acc: 0.9473 - val_loss: 0.4725 - val_acc: 0.9016\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 532us/sample - loss: 0.1429 - acc: 0.9567 - val_loss: 0.2994 - val_acc: 0.9508\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 643us/sample - loss: 0.1181 - acc: 0.9661 - val_loss: 0.2630 - val_acc: 0.9344\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 470us/sample - loss: 0.1148 - acc: 0.9755 - val_loss: 0.2355 - val_acc: 0.9344\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 631us/sample - loss: 0.1084 - acc: 0.9642 - val_loss: 0.2010 - val_acc: 0.9344\n",
      "Fold:  11\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90        97\n",
      "           1       0.96      0.95      0.96       173\n",
      "           2       0.92      0.85      0.88        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.92      0.91      0.91       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 1s 2ms/sample - loss: 0.7092 - acc: 0.7288 - val_loss: 1.1020 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 319us/sample - loss: 0.2651 - acc: 0.9209 - val_loss: 1.0905 - val_acc: 0.6721\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 364us/sample - loss: 0.1875 - acc: 0.9397 - val_loss: 0.7500 - val_acc: 0.6721\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 294us/sample - loss: 0.1622 - acc: 0.9473 - val_loss: 0.8430 - val_acc: 0.6393\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 267us/sample - loss: 0.1565 - acc: 0.9529 - val_loss: 0.7093 - val_acc: 0.6393\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 0s 273us/sample - loss: 0.1358 - acc: 0.9567 - val_loss: 0.5135 - val_acc: 0.7705\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 309us/sample - loss: 0.1394 - acc: 0.9605 - val_loss: 0.4306 - val_acc: 0.8689\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 258us/sample - loss: 0.1160 - acc: 0.9718 - val_loss: 0.2319 - val_acc: 0.9180\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 279us/sample - loss: 0.1246 - acc: 0.9661 - val_loss: 0.2438 - val_acc: 0.9180\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 265us/sample - loss: 0.1091 - acc: 0.9736 - val_loss: 0.2150 - val_acc: 0.9180\n",
      "Fold:  12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89        97\n",
      "           1       0.95      0.96      0.96       173\n",
      "           2       1.00      0.59      0.74        27\n",
      "\n",
      "    accuracy                           0.92       297\n",
      "   macro avg       0.93      0.83      0.86       297\n",
      "weighted avg       0.92      0.92      0.92       297\n",
      "\n",
      "Train on 531 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 1s 2ms/sample - loss: 0.4430 - acc: 0.8249 - val_loss: 1.4991 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 285us/sample - loss: 0.2376 - acc: 0.9266 - val_loss: 1.0994 - val_acc: 0.6557\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 405us/sample - loss: 0.1960 - acc: 0.9322 - val_loss: 1.2111 - val_acc: 0.6721\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 569us/sample - loss: 0.1921 - acc: 0.9416 - val_loss: 0.7803 - val_acc: 0.6721\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 681us/sample - loss: 0.1465 - acc: 0.9623 - val_loss: 0.4854 - val_acc: 0.8525\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - ETA: 0s - loss: 0.1432 - acc: 0.961 - 0s 376us/sample - loss: 0.1484 - acc: 0.9586 - val_loss: 0.5745 - val_acc: 0.7213\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 290us/sample - loss: 0.1358 - acc: 0.9623 - val_loss: 0.3972 - val_acc: 0.8852\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 274us/sample - loss: 0.1216 - acc: 0.9680 - val_loss: 0.2469 - val_acc: 0.9180\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 263us/sample - loss: 0.1237 - acc: 0.9661 - val_loss: 0.2334 - val_acc: 0.9508\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 320us/sample - loss: 0.1074 - acc: 0.9586 - val_loss: 0.1979 - val_acc: 0.9344\n",
      "Fold:  13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91        97\n",
      "           1       0.94      0.97      0.95       173\n",
      "           2       0.86      0.89      0.87        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.91      0.91      0.91       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 532 samples, validate on 60 samples\n",
      "Epoch 1/10\n",
      "532/532 [==============================] - 1s 2ms/sample - loss: 0.5501 - acc: 0.8026 - val_loss: 1.1300 - val_acc: 0.6000\n",
      "Epoch 2/10\n",
      "532/532 [==============================] - 0s 443us/sample - loss: 0.2128 - acc: 0.9342 - val_loss: 1.0671 - val_acc: 0.5667\n",
      "Epoch 3/10\n",
      "532/532 [==============================] - 0s 372us/sample - loss: 0.1726 - acc: 0.9492 - val_loss: 0.9241 - val_acc: 0.6167\n",
      "Epoch 4/10\n",
      "532/532 [==============================] - 0s 272us/sample - loss: 0.1514 - acc: 0.9511 - val_loss: 0.7666 - val_acc: 0.6167\n",
      "Epoch 5/10\n",
      "532/532 [==============================] - 0s 705us/sample - loss: 0.1292 - acc: 0.9586 - val_loss: 0.6964 - val_acc: 0.7000\n",
      "Epoch 6/10\n",
      "532/532 [==============================] - 0s 278us/sample - loss: 0.1063 - acc: 0.9643 - val_loss: 0.5074 - val_acc: 0.8167\n",
      "Epoch 7/10\n",
      "532/532 [==============================] - 0s 315us/sample - loss: 0.1024 - acc: 0.9718 - val_loss: 0.5048 - val_acc: 0.8333\n",
      "Epoch 8/10\n",
      "532/532 [==============================] - 0s 672us/sample - loss: 0.1001 - acc: 0.9662 - val_loss: 0.3808 - val_acc: 0.8667\n",
      "Epoch 9/10\n",
      "532/532 [==============================] - 0s 398us/sample - loss: 0.1087 - acc: 0.9662 - val_loss: 0.3428 - val_acc: 0.8667\n",
      "Epoch 10/10\n",
      "532/532 [==============================] - 0s 330us/sample - loss: 0.1005 - acc: 0.9699 - val_loss: 0.3306 - val_acc: 0.8833\n",
      "Fold:  14\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92        97\n",
      "           1       0.98      0.94      0.96       173\n",
      "           2       0.86      0.89      0.87        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.91      0.92      0.92       297\n",
      "weighted avg       0.94      0.94      0.94       297\n",
      "\n",
      "Train on 533 samples, validate on 59 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 1s 2ms/sample - loss: 0.4775 - acc: 0.7992 - val_loss: 1.5904 - val_acc: 0.3729\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 404us/sample - loss: 0.2195 - acc: 0.9362 - val_loss: 1.5681 - val_acc: 0.3220\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 0s 400us/sample - loss: 0.1941 - acc: 0.9381 - val_loss: 1.1556 - val_acc: 0.6441\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 274us/sample - loss: 0.1694 - acc: 0.9493 - val_loss: 1.0645 - val_acc: 0.4576\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 0s 590us/sample - loss: 0.1640 - acc: 0.9437 - val_loss: 0.5795 - val_acc: 0.7119\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 305us/sample - loss: 0.1546 - acc: 0.9644 - val_loss: 0.4255 - val_acc: 0.8983\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 0s 367us/sample - loss: 0.1225 - acc: 0.9531 - val_loss: 0.4171 - val_acc: 0.8814\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 0s 557us/sample - loss: 0.1234 - acc: 0.9644 - val_loss: 0.2085 - val_acc: 0.9831\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 0s 539us/sample - loss: 0.1382 - acc: 0.9606 - val_loss: 0.1501 - val_acc: 0.9831\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 0s 335us/sample - loss: 0.1032 - acc: 0.9719 - val_loss: 0.1135 - val_acc: 0.9831\n",
      "Fold:  15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91        97\n",
      "           1       0.92      0.96      0.94       173\n",
      "           2       1.00      0.85      0.92        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.95      0.90      0.92       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 1s 2ms/sample - loss: 0.4703 - acc: 0.8202 - val_loss: 1.6453 - val_acc: 0.4138\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 455us/sample - loss: 0.2184 - acc: 0.9307 - val_loss: 0.9242 - val_acc: 0.6552\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 536us/sample - loss: 0.1647 - acc: 0.9588 - val_loss: 1.0245 - val_acc: 0.6207\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 606us/sample - loss: 0.1683 - acc: 0.9513 - val_loss: 1.0142 - val_acc: 0.6207\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 395us/sample - loss: 0.1406 - acc: 0.9625 - val_loss: 0.7234 - val_acc: 0.6552\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 790us/sample - loss: 0.1328 - acc: 0.9644 - val_loss: 0.6482 - val_acc: 0.7414\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 510us/sample - loss: 0.1206 - acc: 0.9663 - val_loss: 0.4638 - val_acc: 0.8448\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 371us/sample - loss: 0.1097 - acc: 0.9663 - val_loss: 0.3363 - val_acc: 0.8966\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 377us/sample - loss: 0.1057 - acc: 0.9700 - val_loss: 0.3400 - val_acc: 0.9310\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 274us/sample - loss: 0.1036 - acc: 0.9719 - val_loss: 0.2636 - val_acc: 0.9310\n",
      "Fold:  16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        97\n",
      "           1       0.96      0.95      0.95       173\n",
      "           2       0.86      0.93      0.89        27\n",
      "\n",
      "    accuracy                           0.94       297\n",
      "   macro avg       0.91      0.93      0.92       297\n",
      "weighted avg       0.94      0.94      0.94       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 2s 3ms/sample - loss: 0.4624 - acc: 0.8127 - val_loss: 1.6282 - val_acc: 0.6552\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 350us/sample - loss: 0.1812 - acc: 0.9438 - val_loss: 1.4034 - val_acc: 0.6552\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 321us/sample - loss: 0.1896 - acc: 0.9307 - val_loss: 1.6306 - val_acc: 0.3621\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 727us/sample - loss: 0.1664 - acc: 0.9532 - val_loss: 1.2866 - val_acc: 0.2241\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 296us/sample - loss: 0.1553 - acc: 0.9438 - val_loss: 0.8896 - val_acc: 0.6379\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 511us/sample - loss: 0.1347 - acc: 0.9644 - val_loss: 0.6138 - val_acc: 0.7069\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 298us/sample - loss: 0.1371 - acc: 0.9625 - val_loss: 0.3879 - val_acc: 0.8448\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 268us/sample - loss: 0.1135 - acc: 0.9663 - val_loss: 0.2453 - val_acc: 0.9483\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 263us/sample - loss: 0.1193 - acc: 0.9663 - val_loss: 0.1791 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 262us/sample - loss: 0.1141 - acc: 0.9682 - val_loss: 0.1619 - val_acc: 0.9655\n",
      "Fold:  17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91        97\n",
      "           1       0.93      0.96      0.95       173\n",
      "           2       0.92      0.89      0.91        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.93      0.91      0.92       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 2s 3ms/sample - loss: 0.4002 - acc: 0.8483 - val_loss: 2.6655 - val_acc: 0.0862\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 642us/sample - loss: 0.2007 - acc: 0.9382 - val_loss: 1.5998 - val_acc: 0.0862\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 0s 354us/sample - loss: 0.1848 - acc: 0.9457 - val_loss: 1.5775 - val_acc: 0.1379\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 351us/sample - loss: 0.1549 - acc: 0.9551 - val_loss: 1.3053 - val_acc: 0.2759\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 381us/sample - loss: 0.1341 - acc: 0.9625 - val_loss: 0.8243 - val_acc: 0.6207\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 339us/sample - loss: 0.1433 - acc: 0.9569 - val_loss: 0.4814 - val_acc: 0.9138\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 358us/sample - loss: 0.1251 - acc: 0.9625 - val_loss: 0.3896 - val_acc: 0.9138\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 372us/sample - loss: 0.0992 - acc: 0.9719 - val_loss: 0.1947 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 363us/sample - loss: 0.1182 - acc: 0.9682 - val_loss: 0.1827 - val_acc: 0.9483\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 399us/sample - loss: 0.0880 - acc: 0.9775 - val_loss: 0.1115 - val_acc: 0.9655\n",
      "Fold:  18\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91        97\n",
      "           1       0.94      0.96      0.95       173\n",
      "           2       1.00      0.81      0.90        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.95      0.90      0.92       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 2s 3ms/sample - loss: 0.5516 - acc: 0.8090 - val_loss: 1.1119 - val_acc: 0.4828\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 0s 357us/sample - loss: 0.2126 - acc: 0.9363 - val_loss: 0.9613 - val_acc: 0.6379\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/534 [==============================] - 0s 355us/sample - loss: 0.2019 - acc: 0.9419 - val_loss: 0.9048 - val_acc: 0.6379\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 456us/sample - loss: 0.1769 - acc: 0.9401 - val_loss: 0.7393 - val_acc: 0.6379\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 466us/sample - loss: 0.1551 - acc: 0.9513 - val_loss: 0.6533 - val_acc: 0.6552\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 517us/sample - loss: 0.1426 - acc: 0.9569 - val_loss: 0.4078 - val_acc: 0.9655\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 509us/sample - loss: 0.1561 - acc: 0.9513 - val_loss: 0.3584 - val_acc: 0.9483\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 0s 435us/sample - loss: 0.1296 - acc: 0.9644 - val_loss: 0.3073 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 0s 781us/sample - loss: 0.1404 - acc: 0.9569 - val_loss: 0.2510 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 392us/sample - loss: 0.1163 - acc: 0.9663 - val_loss: 0.1669 - val_acc: 0.9655\n",
      "Fold:  19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        97\n",
      "           1       0.94      0.97      0.95       173\n",
      "           2       1.00      0.74      0.85        27\n",
      "\n",
      "    accuracy                           0.93       297\n",
      "   macro avg       0.95      0.88      0.91       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Train on 534 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "534/534 [==============================] - 8s 15ms/sample - loss: 0.5353 - acc: 0.7697 - val_loss: 1.0639 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "534/534 [==============================] - 1s 1ms/sample - loss: 0.2192 - acc: 0.9419 - val_loss: 0.9538 - val_acc: 0.6379\n",
      "Epoch 3/10\n",
      "534/534 [==============================] - 1s 1ms/sample - loss: 0.1857 - acc: 0.9494 - val_loss: 0.8434 - val_acc: 0.6379\n",
      "Epoch 4/10\n",
      "534/534 [==============================] - 0s 598us/sample - loss: 0.1658 - acc: 0.9494 - val_loss: 0.8670 - val_acc: 0.6552\n",
      "Epoch 5/10\n",
      "534/534 [==============================] - 0s 699us/sample - loss: 0.1463 - acc: 0.9513 - val_loss: 0.6312 - val_acc: 0.6724\n",
      "Epoch 6/10\n",
      "534/534 [==============================] - 0s 687us/sample - loss: 0.1391 - acc: 0.9588 - val_loss: 0.5849 - val_acc: 0.8621\n",
      "Epoch 7/10\n",
      "534/534 [==============================] - 0s 789us/sample - loss: 0.1274 - acc: 0.9588 - val_loss: 0.4338 - val_acc: 0.9310\n",
      "Epoch 8/10\n",
      "534/534 [==============================] - 1s 963us/sample - loss: 0.1107 - acc: 0.9682 - val_loss: 0.3262 - val_acc: 0.9138\n",
      "Epoch 9/10\n",
      "534/534 [==============================] - 1s 1ms/sample - loss: 0.0979 - acc: 0.9682 - val_loss: 0.2656 - val_acc: 0.9138\n",
      "Epoch 10/10\n",
      "534/534 [==============================] - 0s 919us/sample - loss: 0.1049 - acc: 0.9682 - val_loss: 0.2507 - val_acc: 0.9483\n",
      "Fold:  20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92        97\n",
      "           1       0.90      0.98      0.94       173\n",
      "           2       1.00      0.67      0.80        27\n",
      "\n",
      "    accuracy                           0.92       297\n",
      "   macro avg       0.95      0.85      0.89       297\n",
      "weighted avg       0.93      0.92      0.92       297\n",
      "\n",
      "Train on 533 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 2s 4ms/sample - loss: 0.4413 - acc: 0.8199 - val_loss: 1.0389 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 503us/sample - loss: 0.2365 - acc: 0.9174 - val_loss: 0.9540 - val_acc: 0.6557\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 0s 359us/sample - loss: 0.2139 - acc: 0.9325 - val_loss: 0.7269 - val_acc: 0.6557\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 438us/sample - loss: 0.1779 - acc: 0.9400 - val_loss: 0.8778 - val_acc: 0.6557\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 0s 756us/sample - loss: 0.1701 - acc: 0.9512 - val_loss: 0.5216 - val_acc: 0.7377\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 473us/sample - loss: 0.1570 - acc: 0.9531 - val_loss: 0.3699 - val_acc: 0.9180\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 0s 471us/sample - loss: 0.1404 - acc: 0.9568 - val_loss: 0.3119 - val_acc: 0.9180\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 0s 459us/sample - loss: 0.1439 - acc: 0.9531 - val_loss: 0.2407 - val_acc: 0.9672\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 0s 343us/sample - loss: 0.1278 - acc: 0.9493 - val_loss: 0.1875 - val_acc: 0.9672\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 0s 404us/sample - loss: 0.1272 - acc: 0.9531 - val_loss: 0.1693 - val_acc: 0.9508\n",
      "Fold:  21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94        96\n",
      "           1       0.97      0.98      0.97       172\n",
      "           2       0.86      0.93      0.89        27\n",
      "\n",
      "    accuracy                           0.96       295\n",
      "   macro avg       0.93      0.94      0.94       295\n",
      "weighted avg       0.96      0.96      0.96       295\n",
      "\n",
      "Train on 533 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 2s 4ms/sample - loss: 0.5564 - acc: 0.7992 - val_loss: 1.2074 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 538us/sample - loss: 0.2451 - acc: 0.9174 - val_loss: 1.0947 - val_acc: 0.6393\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 0s 565us/sample - loss: 0.1894 - acc: 0.9325 - val_loss: 1.0579 - val_acc: 0.6066\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 562us/sample - loss: 0.1705 - acc: 0.9456 - val_loss: 0.9544 - val_acc: 0.6393\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 1s 958us/sample - loss: 0.1475 - acc: 0.9493 - val_loss: 0.7140 - val_acc: 0.6230\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 544us/sample - loss: 0.1523 - acc: 0.9512 - val_loss: 0.6331 - val_acc: 0.6721\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 0s 915us/sample - loss: 0.1344 - acc: 0.9606 - val_loss: 0.4144 - val_acc: 0.9344\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 0s 614us/sample - loss: 0.1211 - acc: 0.9587 - val_loss: 0.3799 - val_acc: 0.8689\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 0s 654us/sample - loss: 0.1325 - acc: 0.9550 - val_loss: 0.2625 - val_acc: 0.9180\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 0s 330us/sample - loss: 0.1128 - acc: 0.9662 - val_loss: 0.2201 - val_acc: 0.9508\n",
      "Fold:  22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95        96\n",
      "           1       0.97      0.99      0.98       172\n",
      "           2       0.84      1.00      0.92        27\n",
      "\n",
      "    accuracy                           0.96       295\n",
      "   macro avg       0.93      0.96      0.95       295\n",
      "weighted avg       0.97      0.96      0.96       295\n",
      "\n",
      "Train on 533 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 9s 17ms/sample - loss: 0.4963 - acc: 0.8124 - val_loss: 1.2706 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 756us/sample - loss: 0.2223 - acc: 0.9287 - val_loss: 1.7781 - val_acc: 0.0984\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 1s 1ms/sample - loss: 0.1921 - acc: 0.9456 - val_loss: 1.7960 - val_acc: 0.0984\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 768us/sample - loss: 0.1503 - acc: 0.9568 - val_loss: 0.9140 - val_acc: 0.6721\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 1s 968us/sample - loss: 0.1531 - acc: 0.9493 - val_loss: 0.9197 - val_acc: 0.6557\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 850us/sample - loss: 0.1376 - acc: 0.9550 - val_loss: 0.5046 - val_acc: 0.8033\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 1s 2ms/sample - loss: 0.1244 - acc: 0.9644 - val_loss: 0.3106 - val_acc: 0.9344\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 1s 1ms/sample - loss: 0.1187 - acc: 0.9606 - val_loss: 0.2904 - val_acc: 0.9344\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 1s 1ms/sample - loss: 0.1282 - acc: 0.9662 - val_loss: 0.2628 - val_acc: 0.9344\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 1s 1ms/sample - loss: 0.0932 - acc: 0.9681 - val_loss: 0.2307 - val_acc: 0.9344\n",
      "Fold:  23\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        96\n",
      "           1       0.96      0.99      0.97       172\n",
      "           2       0.89      0.89      0.89        27\n",
      "\n",
      "    accuracy                           0.96       295\n",
      "   macro avg       0.95      0.94      0.94       295\n",
      "weighted avg       0.96      0.96      0.96       295\n",
      "\n",
      "Train on 533 samples, validate on 61 samples\n",
      "Epoch 1/10\n",
      "533/533 [==============================] - 6s 11ms/sample - loss: 0.6239 - acc: 0.7561 - val_loss: 1.0833 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "533/533 [==============================] - 0s 511us/sample - loss: 0.2834 - acc: 0.9137 - val_loss: 1.2836 - val_acc: 0.0984\n",
      "Epoch 3/10\n",
      "533/533 [==============================] - 0s 457us/sample - loss: 0.2135 - acc: 0.9231 - val_loss: 1.3232 - val_acc: 0.0984\n",
      "Epoch 4/10\n",
      "533/533 [==============================] - 0s 519us/sample - loss: 0.1840 - acc: 0.9362 - val_loss: 0.9585 - val_acc: 0.6393\n",
      "Epoch 5/10\n",
      "533/533 [==============================] - 0s 834us/sample - loss: 0.1902 - acc: 0.9306 - val_loss: 1.0516 - val_acc: 0.3607\n",
      "Epoch 6/10\n",
      "533/533 [==============================] - 0s 738us/sample - loss: 0.1589 - acc: 0.9568 - val_loss: 0.7079 - val_acc: 0.6885\n",
      "Epoch 7/10\n",
      "533/533 [==============================] - 0s 773us/sample - loss: 0.1431 - acc: 0.9550 - val_loss: 0.4511 - val_acc: 0.8525\n",
      "Epoch 8/10\n",
      "533/533 [==============================] - 0s 754us/sample - loss: 0.1509 - acc: 0.9437 - val_loss: 0.3138 - val_acc: 0.9344\n",
      "Epoch 9/10\n",
      "533/533 [==============================] - 0s 739us/sample - loss: 0.1461 - acc: 0.9606 - val_loss: 0.3273 - val_acc: 0.9180\n",
      "Epoch 10/10\n",
      "533/533 [==============================] - 0s 921us/sample - loss: 0.1245 - acc: 0.9681 - val_loss: 0.2978 - val_acc: 0.9180\n",
      "Fold:  24\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.91        96\n",
      "           1       0.98      0.95      0.96       172\n",
      "           2       0.63      1.00      0.77        27\n",
      "\n",
      "    accuracy                           0.92       295\n",
      "   macro avg       0.86      0.93      0.88       295\n",
      "weighted avg       0.94      0.92      0.93       295\n",
      "\n",
      "Train on 535 samples, validate on 59 samples\n",
      "Epoch 1/10\n",
      "535/535 [==============================] - 4s 8ms/sample - loss: 0.5347 - acc: 0.8093 - val_loss: 1.1803 - val_acc: 0.5932\n",
      "Epoch 2/10\n",
      "535/535 [==============================] - 0s 522us/sample - loss: 0.2613 - acc: 0.9159 - val_loss: 0.8573 - val_acc: 0.6610\n",
      "Epoch 3/10\n",
      "535/535 [==============================] - 0s 472us/sample - loss: 0.2250 - acc: 0.9346 - val_loss: 0.7192 - val_acc: 0.6610\n",
      "Epoch 4/10\n",
      "535/535 [==============================] - 0s 468us/sample - loss: 0.2065 - acc: 0.9383 - val_loss: 0.7466 - val_acc: 0.6610\n",
      "Epoch 5/10\n",
      "535/535 [==============================] - 0s 429us/sample - loss: 0.1896 - acc: 0.9421 - val_loss: 0.6286 - val_acc: 0.6610\n",
      "Epoch 6/10\n",
      "535/535 [==============================] - 0s 529us/sample - loss: 0.1581 - acc: 0.9421 - val_loss: 0.5886 - val_acc: 0.7119\n",
      "Epoch 7/10\n",
      "535/535 [==============================] - 0s 902us/sample - loss: 0.1561 - acc: 0.9495 - val_loss: 0.4052 - val_acc: 0.8644\n",
      "Epoch 8/10\n",
      "535/535 [==============================] - 0s 682us/sample - loss: 0.1451 - acc: 0.9533 - val_loss: 0.3576 - val_acc: 0.9322\n",
      "Epoch 9/10\n",
      "535/535 [==============================] - 0s 690us/sample - loss: 0.1412 - acc: 0.9514 - val_loss: 0.3018 - val_acc: 0.9153\n",
      "Epoch 10/10\n",
      "535/535 [==============================] - 0s 552us/sample - loss: 0.1335 - acc: 0.9589 - val_loss: 0.2327 - val_acc: 0.9492\n",
      "Fold:  25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92        96\n",
      "           1       0.95      0.99      0.97       172\n",
      "           2       0.81      0.96      0.88        27\n",
      "\n",
      "    accuracy                           0.95       295\n",
      "   macro avg       0.92      0.94      0.92       295\n",
      "weighted avg       0.95      0.95      0.95       295\n",
      "\n",
      "Train on 535 samples, validate on 59 samples\n",
      "Epoch 1/10\n",
      "535/535 [==============================] - 3s 6ms/sample - loss: 0.4540 - acc: 0.8336 - val_loss: 1.4830 - val_acc: 0.6441\n",
      "Epoch 2/10\n",
      "535/535 [==============================] - 1s 1ms/sample - loss: 0.2123 - acc: 0.9308 - val_loss: 1.3010 - val_acc: 0.6610\n",
      "Epoch 3/10\n",
      "535/535 [==============================] - 0s 753us/sample - loss: 0.1805 - acc: 0.9383 - val_loss: 1.0218 - val_acc: 0.6441\n",
      "Epoch 4/10\n",
      "535/535 [==============================] - 0s 836us/sample - loss: 0.1862 - acc: 0.9308 - val_loss: 1.0273 - val_acc: 0.6780\n",
      "Epoch 5/10\n",
      "535/535 [==============================] - 0s 497us/sample - loss: 0.1683 - acc: 0.9421 - val_loss: 0.6732 - val_acc: 0.6780\n",
      "Epoch 6/10\n",
      "535/535 [==============================] - 0s 414us/sample - loss: 0.1766 - acc: 0.9402 - val_loss: 0.4959 - val_acc: 0.8305\n",
      "Epoch 7/10\n",
      "535/535 [==============================] - 0s 438us/sample - loss: 0.1596 - acc: 0.9514 - val_loss: 0.3890 - val_acc: 0.8644\n",
      "Epoch 8/10\n",
      "535/535 [==============================] - 0s 661us/sample - loss: 0.1403 - acc: 0.9570 - val_loss: 0.2521 - val_acc: 0.9831\n",
      "Epoch 9/10\n",
      "535/535 [==============================] - 0s 704us/sample - loss: 0.1234 - acc: 0.9570 - val_loss: 0.1777 - val_acc: 0.9661\n",
      "Epoch 10/10\n",
      "535/535 [==============================] - 0s 828us/sample - loss: 0.1298 - acc: 0.9477 - val_loss: 0.1351 - val_acc: 0.9831\n",
      "Fold:  26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        96\n",
      "           1       0.97      0.97      0.97       172\n",
      "           2       0.89      0.89      0.89        27\n",
      "\n",
      "    accuracy                           0.95       295\n",
      "   macro avg       0.93      0.93      0.93       295\n",
      "weighted avg       0.95      0.95      0.95       295\n",
      "\n",
      "Train on 536 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "536/536 [==============================] - 4s 7ms/sample - loss: 0.5070 - acc: 0.7985 - val_loss: 1.3570 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 0s 547us/sample - loss: 0.2527 - acc: 0.9216 - val_loss: 1.1853 - val_acc: 0.5862\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 0s 500us/sample - loss: 0.2273 - acc: 0.9291 - val_loss: 0.9571 - val_acc: 0.6379\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 0s 464us/sample - loss: 0.1815 - acc: 0.9403 - val_loss: 0.8084 - val_acc: 0.6724\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 0s 467us/sample - loss: 0.1802 - acc: 0.9478 - val_loss: 0.6533 - val_acc: 0.7414\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 0s 550us/sample - loss: 0.1691 - acc: 0.9403 - val_loss: 0.3332 - val_acc: 0.9655\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 0s 756us/sample - loss: 0.1465 - acc: 0.9552 - val_loss: 0.2672 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 0s 932us/sample - loss: 0.1309 - acc: 0.9552 - val_loss: 0.2335 - val_acc: 0.9828\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 0s 727us/sample - loss: 0.1276 - acc: 0.9571 - val_loss: 0.1713 - val_acc: 0.9655\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 0s 733us/sample - loss: 0.1145 - acc: 0.9608 - val_loss: 0.1424 - val_acc: 0.9483\n",
      "Fold:  27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.93        96\n",
      "           1       0.98      0.98      0.98       172\n",
      "           2       0.81      0.93      0.86        27\n",
      "\n",
      "    accuracy                           0.95       295\n",
      "   macro avg       0.91      0.94      0.92       295\n",
      "weighted avg       0.95      0.95      0.95       295\n",
      "\n",
      "Train on 536 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "536/536 [==============================] - 4s 8ms/sample - loss: 0.4362 - acc: 0.8451 - val_loss: 1.5304 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1945 - acc: 0.9254 - val_loss: 1.1996 - val_acc: 0.6207\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 0s 589us/sample - loss: 0.1827 - acc: 0.9478 - val_loss: 1.0693 - val_acc: 0.6207\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536/536 [==============================] - 0s 459us/sample - loss: 0.1523 - acc: 0.9478 - val_loss: 0.8089 - val_acc: 0.6207\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 0s 437us/sample - loss: 0.1456 - acc: 0.9478 - val_loss: 0.6520 - val_acc: 0.6379\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 0s 597us/sample - loss: 0.1355 - acc: 0.9534 - val_loss: 0.5325 - val_acc: 0.6897\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 0s 471us/sample - loss: 0.1084 - acc: 0.9608 - val_loss: 0.4914 - val_acc: 0.7586\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 0s 385us/sample - loss: 0.1067 - acc: 0.9646 - val_loss: 0.3476 - val_acc: 0.8793\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 0s 362us/sample - loss: 0.1227 - acc: 0.9571 - val_loss: 0.3401 - val_acc: 0.8966\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 0s 355us/sample - loss: 0.0923 - acc: 0.9664 - val_loss: 0.3441 - val_acc: 0.8966\n",
      "Fold:  28\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94        96\n",
      "           1       0.97      0.97      0.97       172\n",
      "           2       0.89      0.93      0.91        27\n",
      "\n",
      "    accuracy                           0.96       295\n",
      "   macro avg       0.94      0.94      0.94       295\n",
      "weighted avg       0.96      0.96      0.96       295\n",
      "\n",
      "Train on 536 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "536/536 [==============================] - 8s 14ms/sample - loss: 0.4633 - acc: 0.8209 - val_loss: 1.1172 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 0s 821us/sample - loss: 0.2321 - acc: 0.9310 - val_loss: 1.4896 - val_acc: 0.6379\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1805 - acc: 0.9478 - val_loss: 1.2506 - val_acc: 0.6034\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1834 - acc: 0.9403 - val_loss: 1.0853 - val_acc: 0.5862\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 1s 982us/sample - loss: 0.1604 - acc: 0.9459 - val_loss: 0.8874 - val_acc: 0.5862\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 1s 970us/sample - loss: 0.1436 - acc: 0.9552 - val_loss: 0.5124 - val_acc: 0.8103\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1385 - acc: 0.9552 - val_loss: 0.4760 - val_acc: 0.8103\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 1s 944us/sample - loss: 0.1292 - acc: 0.9590 - val_loss: 0.3634 - val_acc: 0.8621\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 0s 836us/sample - loss: 0.1152 - acc: 0.9664 - val_loss: 0.3423 - val_acc: 0.8793\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 0s 475us/sample - loss: 0.1239 - acc: 0.9515 - val_loss: 0.2644 - val_acc: 0.9138\n",
      "Fold:  29\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95        96\n",
      "           1       0.96      0.99      0.97       172\n",
      "           2       0.86      0.93      0.89        27\n",
      "\n",
      "    accuracy                           0.96       295\n",
      "   macro avg       0.94      0.94      0.94       295\n",
      "weighted avg       0.96      0.96      0.96       295\n",
      "\n",
      "Train on 536 samples, validate on 58 samples\n",
      "Epoch 1/10\n",
      "536/536 [==============================] - 8s 14ms/sample - loss: 0.7348 - acc: 0.6810 - val_loss: 0.9394 - val_acc: 0.5862\n",
      "Epoch 2/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.2987 - acc: 0.9049 - val_loss: 0.9039 - val_acc: 0.6207\n",
      "Epoch 3/10\n",
      "536/536 [==============================] - 0s 780us/sample - loss: 0.2155 - acc: 0.9216 - val_loss: 0.9006 - val_acc: 0.6207\n",
      "Epoch 4/10\n",
      "536/536 [==============================] - 0s 898us/sample - loss: 0.1883 - acc: 0.9440 - val_loss: 0.7387 - val_acc: 0.6207\n",
      "Epoch 5/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1576 - acc: 0.9496 - val_loss: 0.6739 - val_acc: 0.7586\n",
      "Epoch 6/10\n",
      "536/536 [==============================] - 1s 1ms/sample - loss: 0.1517 - acc: 0.9496 - val_loss: 0.5091 - val_acc: 0.8448\n",
      "Epoch 7/10\n",
      "536/536 [==============================] - 0s 818us/sample - loss: 0.1348 - acc: 0.9515 - val_loss: 0.5152 - val_acc: 0.8276\n",
      "Epoch 8/10\n",
      "536/536 [==============================] - 0s 569us/sample - loss: 0.1448 - acc: 0.9552 - val_loss: 0.3608 - val_acc: 0.9138\n",
      "Epoch 9/10\n",
      "536/536 [==============================] - 0s 631us/sample - loss: 0.1270 - acc: 0.9627 - val_loss: 0.2997 - val_acc: 0.9310\n",
      "Epoch 10/10\n",
      "536/536 [==============================] - 0s 773us/sample - loss: 0.1274 - acc: 0.9590 - val_loss: 0.2806 - val_acc: 0.9138\n",
      "Fold:  30\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        96\n",
      "           1       0.98      0.97      0.97       172\n",
      "           2       0.86      0.93      0.89        27\n",
      "\n",
      "    accuracy                           0.95       295\n",
      "   macro avg       0.92      0.94      0.93       295\n",
      "weighted avg       0.95      0.95      0.95       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "kfold1 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "kfold2 = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "\n",
    "for train_idx_1, test_idx_1 in kfold2.split(X.T, y):\n",
    "    X_train, X_test = X.T[train_idx_1], X.T[test_idx_1]\n",
    "    X_train, X_test = X_train[..., None], X_test[..., None]\n",
    "    y_train, y_test = y[train_idx_1], y[test_idx_1]\n",
    "    f1_weighted_per_fold = []\n",
    "    f1_macro_per_fold = []\n",
    "    f1_micro_per_fold = []\n",
    "    testacc_per_fold = []\n",
    "    precision_per_fold = []\n",
    "    recall_per_fold = []\n",
    "    for train_ix, test_ix in kfold1.split(X_train, y_train):\n",
    "        train_X, test_X = X_train[train_ix], X_train[test_ix]\n",
    "        train_y, test_y = y_train[train_ix], y_train[test_ix]\n",
    "\n",
    "        N = X_train.shape[-2]      # Number of nodes in the graphs\n",
    "        F = X_train.shape[-1]      # Node features dimensionality\n",
    "\n",
    "        # Model definition\n",
    "        X_in = Input(shape=(N, F))\n",
    "        A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "        # dropout_1 = Dropout(dropout)(X_in)\n",
    "        bn_1 = BatchNormalization()(X_in)\n",
    "        graph_conv_1 = ChebConv(32,\n",
    "                                K=K,\n",
    "                                activation='relu',\n",
    "                                kernel_regularizer=l2(l2_reg),\n",
    "                                use_bias=False)([bn_1, A_in])\n",
    "        # dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "        bn_2 = BatchNormalization()(graph_conv_1)\n",
    "        graph_conv_2 = ChebConv(32,\n",
    "                                K=K,\n",
    "                                activation='relu',\n",
    "                                use_bias=False)([bn_2, A_in])\n",
    "        flatten = Flatten()(graph_conv_2)\n",
    "        fc_1 = Dense(64, activation='relu')(flatten)\n",
    "        dropout_1 = Dropout(0.3, seed=42)(fc_1)\n",
    "        fc_2 = Dense(32, activation='relu')(dropout_1)\n",
    "        output = Dense(n_out, activation='softmax')(fc_2)\n",
    "\n",
    "        # Build model\n",
    "        model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "        optimizer = Adam(lr=learning_rate)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "\n",
    "\n",
    "        # Train model\n",
    "        validation_data = (test_X, test_y)\n",
    "        model.fit(train_X,\n",
    "                  train_y,\n",
    "                  batch_size=16,\n",
    "                  validation_data=validation_data,\n",
    "                  epochs=10)\n",
    "\n",
    "        y_pred = model.predict(X_test, verbose=1)\n",
    "        y_p = []\n",
    "        for row in y_pred:\n",
    "            y_p.append(np.argmax(row))\n",
    "        target_names = ['0', '1', '2']\n",
    "        print(\"Fold: \", fold)\n",
    "        fold += 1\n",
    "        print(classification_report(y_test, y_p, target_names=target_names))\n",
    "        f1_weighted_per_fold.append(f1_score(y_test, y_p, average='weighted'))\n",
    "        f1_macro_per_fold.append(f1_score(y_test, y_p, average='macro'))\n",
    "        f1_micro_per_fold.append(f1_score(y_test, y_p, average='micro'))\n",
    "        testacc_per_fold.append(accuracy_score(y_test, y_p))\n",
    "        precision_per_fold.append(precision_score(y_test, y_p,  average='micro'))\n",
    "        recall_per_fold.append(recall_score(y_test, y_p,  average='micro'))\n",
    "        \n",
    "    f1_weighted_per_level.append(np.mean(f1_weighted_per_fold))\n",
    "    f1_macro_per_level.append(np.mean(f1_macro_per_fold))\n",
    "    f1_micro_per_level.append(np.mean(f1_micro_per_fold))\n",
    "    testacc_per_level.append(np.mean(testacc_per_fold))\n",
    "    precision_per_level.append(np.mean(precision_per_fold))\n",
    "    recall_per_level.append(np.mean(recall_per_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8,5\n",
    "\n",
    "plt.plot(testacc_per_level, color='red', label='test_acc')\n",
    "plt.plot(f1_weighted_per_level, color='blue', label='f1_weighted')\n",
    "plt.plot(f1_macro_per_level, color='green', label='f1_macro')\n",
    "plt.plot(f1_micro_per_level, color='orange', label='f1_micro')\n",
    "plt.plot(precision_per_level, color='black', label='precision')\n",
    "plt.plot(recall_per_level, color='cyan', label='recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 weighted score:  0.9397750400418925\n",
      "Mean F1 macro score:  0.9158703243680885\n",
      "Mean F1 micro score:  0.9393966025604442\n",
      "Mean test accuracy score:  0.9393966025604442\n",
      "Mean precision:  0.9393966025604442\n",
      "Mean recall:  0.9393966025604442\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean F1 weighted score: \", np.mean(f1_weighted_per_level))\n",
    "print(\"Mean F1 macro score: \", np.mean(f1_macro_per_level))\n",
    "print(\"Mean F1 micro score: \", np.mean(f1_micro_per_level))\n",
    "print(\"Mean test accuracy score: \", np.mean(testacc_per_level))\n",
    "print(\"Mean precision: \", np.mean(precision_per_level))\n",
    "print(\"Mean recall: \", np.mean(recall_per_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
